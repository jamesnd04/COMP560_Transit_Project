{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 02 - Feature Engineering\n",
        "\n",
        "This notebook prepares features for model training by:\n",
        "1. Loading and merging vehicle positions with stop_times\n",
        "2. Extracting all features\n",
        "3. Selecting relevant features for delay prediction\n",
        "4. Preparing training data (X, y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import sys\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Add src to path\n",
        "sys.path.append(str(Path('../src').resolve()))\n",
        "from data_utils import (\n",
        "    load_vehicle_positions,\n",
        "    load_gtfs_data,\n",
        "    preprocess_gtfs,\n",
        "    merge_vehicle_positions_with_stop_times\n",
        ")\n",
        "from features import extract_all_features, create_feature_matrix\n",
        "\n",
        "# Set up paths\n",
        "data_dir = Path('../data/raw')\n",
        "processed_dir = Path('../data/processed')\n",
        "processed_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Feature Engineering - Transit Tracker\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load and Process Full Dataset\n",
        "\n",
        "Load vehicle positions and stop_times, then merge them. For faster processing, you can use a sample.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load vehicle positions\n",
        "vehicle_positions_file = 'vehicle_positions_rt_rows-11-13.csv'\n",
        "print(f\"Loading vehicle positions from {vehicle_positions_file}...\")\n",
        "vehicle_positions = load_vehicle_positions(data_dir / vehicle_positions_file)\n",
        "print(f\"Loaded {len(vehicle_positions):,} vehicle position records\")\n",
        "\n",
        "# Option: Use a sample for faster processing during development\n",
        "# Remove this line for full dataset processing\n",
        "USE_SAMPLE = True\n",
        "SAMPLE_SIZE = 50000  # Adjust based on your needs\n",
        "\n",
        "if USE_SAMPLE:\n",
        "    vehicle_positions = vehicle_positions.sample(n=min(SAMPLE_SIZE, len(vehicle_positions)), random_state=42)\n",
        "    print(f\"Using sample of {len(vehicle_positions):,} records\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and preprocess GTFS data\n",
        "print(\"\\nLoading GTFS stop_times...\")\n",
        "gtfs_data = load_gtfs_data(data_dir)\n",
        "gtfs_data = preprocess_gtfs(gtfs_data)\n",
        "\n",
        "if 'stop_times' not in gtfs_data:\n",
        "    raise FileNotFoundError(\"stop_times.txt not found!\")\n",
        "\n",
        "print(f\"Loaded {len(gtfs_data['stop_times']):,} stop_times records\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Merge vehicle positions with stop_times\n",
        "print(\"\\nMerging vehicle positions with stop_times...\")\n",
        "merged_data = merge_vehicle_positions_with_stop_times(\n",
        "    vehicle_positions,\n",
        "    gtfs_data['stop_times']\n",
        ")\n",
        "\n",
        "merge_rate = merged_data['arrival_time'].notna().sum() / len(merged_data) * 100\n",
        "print(f\"Merge success rate: {merge_rate:.1f}%\")\n",
        "print(f\"Merged data shape: {merged_data.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Extract All Features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract all features\n",
        "print(\"\\nExtracting features...\")\n",
        "features_df = extract_all_features(merged_data)\n",
        "\n",
        "print(f\"Features shape: {features_df.shape}\")\n",
        "print(f\"Total feature columns: {len(features_df.columns)}\")\n",
        "\n",
        "# Display feature columns\n",
        "print(\"\\nFeature columns:\")\n",
        "for i, col in enumerate(features_df.columns, 1):\n",
        "    print(f\"{i:3d}. {col}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Select Features for Model Training\n",
        "\n",
        "Select features that are useful for predicting delays. Exclude target variable and non-predictive columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define feature columns for model training\n",
        "# Exclude: target variables, IDs, timestamps, and derived delay columns\n",
        "\n",
        "exclude_cols = [\n",
        "    # Target variables\n",
        "    'arrival_delay_minutes', 'arrival_delay_seconds',\n",
        "    'departure_delay_minutes', 'departure_delay_seconds',\n",
        "    'predicted_delay_minutes',\n",
        "    \n",
        "    # IDs and identifiers\n",
        "    'id', 'trip_id', 'route_id', 'vehicle_id', 'vehicle_label',\n",
        "    'stop_id', 'trip_id_numeric', 'trip_date_suffix',\n",
        "    \n",
        "    # Timestamps and dates\n",
        "    'timestamp', 'datetime', 'start_date',\n",
        "    'scheduled_arrival', 'expected_arrival',\n",
        "    'arrival_time', 'departure_time',\n",
        "    \n",
        "    # Status text (we have encoded versions)\n",
        "    'current_status',\n",
        "    \n",
        "    # Other non-predictive\n",
        "    'schedule_relationship', 'stop_headsign',\n",
        "    'pickup_type', 'drop_off_type',\n",
        "]\n",
        "\n",
        "# Get available feature columns\n",
        "available_cols = [col for col in features_df.columns if col not in exclude_cols]\n",
        "\n",
        "# Select numeric and boolean features\n",
        "feature_cols = []\n",
        "for col in available_cols:\n",
        "    dtype = features_df[col].dtype\n",
        "    if dtype in ['int64', 'float64', 'bool'] or dtype.name == 'category':\n",
        "        feature_cols.append(col)\n",
        "\n",
        "print(f\"\\nSelected {len(feature_cols)} features for training:\")\n",
        "for i, col in enumerate(feature_cols, 1):\n",
        "    print(f\"{i:3d}. {col}\")\n",
        "\n",
        "# Display feature statistics\n",
        "print(\"\\nFeature statistics:\")\n",
        "features_df[feature_cols].describe()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare target variable (arrival delay in minutes)\n",
        "# Use actual delay if available, otherwise skip rows without delay\n",
        "if 'arrival_delay_minutes' in features_df.columns:\n",
        "    # Filter rows with valid delay values\n",
        "    valid_mask = features_df['arrival_delay_minutes'].notna()\n",
        "    training_data = features_df[valid_mask].copy()\n",
        "    \n",
        "    y = training_data['arrival_delay_minutes'].values\n",
        "    X = create_feature_matrix(training_data, feature_cols)\n",
        "    \n",
        "    print(f\"Training data shape: X={X.shape}, y={y.shape}\")\n",
        "    print(f\"\\nTarget variable (y) statistics:\")\n",
        "    print(f\"  Mean: {y.mean():.2f} minutes\")\n",
        "    print(f\"  Median: {np.median(y):.2f} minutes\")\n",
        "    print(f\"  Std: {y.std():.2f} minutes\")\n",
        "    print(f\"  Min: {y.min():.2f} minutes\")\n",
        "    print(f\"  Max: {y.max():.2f} minutes\")\n",
        "    \n",
        "    # Check for any NaN or Inf values\n",
        "    print(f\"\\nData quality check:\")\n",
        "    print(f\"  NaN in X: {np.isnan(X).sum()}\")\n",
        "    print(f\"  Inf in X: {np.isinf(X).sum()}\")\n",
        "    print(f\"  NaN in y: {np.isnan(y).sum()}\")\n",
        "else:\n",
        "    print(\"Warning: No arrival_delay_minutes column found!\")\n",
        "    print(\"Make sure you've merged vehicle positions with stop_times data.\")\n",
        "    X, y = None, None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Save Training Data\n",
        "\n",
        "Save the feature matrix, target variable, and feature column names for model training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if X is not None and y is not None:\n",
        "    # Create DataFrame with features and target\n",
        "    training_df = pd.DataFrame(X, columns=feature_cols)\n",
        "    training_df['arrival_delay_minutes'] = y\n",
        "    \n",
        "    # Save training data\n",
        "    training_data_path = processed_dir / 'training_data.csv'\n",
        "    training_df.to_csv(training_data_path, index=False)\n",
        "    print(f\"\\nSaved training data to: {training_data_path}\")\n",
        "    print(f\"Shape: {training_df.shape}\")\n",
        "    \n",
        "    # Save feature column names for later use\n",
        "    import json\n",
        "    feature_cols_path = processed_dir / 'feature_columns.json'\n",
        "    with open(feature_cols_path, 'w') as f:\n",
        "        json.dump(feature_cols, f)\n",
        "    print(f\"Saved feature columns to: {feature_cols_path}\")\n",
        "    \n",
        "    # Also save the full features DataFrame for reference\n",
        "    features_path = processed_dir / 'all_features.csv'\n",
        "    features_df.to_csv(features_path, index=False)\n",
        "    print(f\"Saved full features DataFrame to: {features_path}\")\n",
        "else:\n",
        "    print(\"\\nCannot save training data - missing X or y\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
