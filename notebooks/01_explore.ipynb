{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 01 - Data Exploration\n",
        "\n",
        "This notebook explores the vehicle positions and GTFS transit data.\n",
        "\n",
        "## Steps:\n",
        "1. Load vehicle positions CSV\n",
        "2. Load GTFS stop_times data\n",
        "3. Explore data structure and basic statistics\n",
        "4. Visualize delays and patterns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "import sys\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Add src to path\n",
        "sys.path.append(str(Path('../src').resolve()))\n",
        "from data_utils import (\n",
        "    load_gtfs_data, \n",
        "    load_vehicle_positions,\n",
        "    preprocess_gtfs,\n",
        "    merge_vehicle_positions_with_stop_times\n",
        ")\n",
        "from features import extract_all_features\n",
        "\n",
        "# Set up paths\n",
        "data_dir = Path('../data/raw')\n",
        "processed_dir = Path('../data/processed')\n",
        "processed_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Data Exploration - Transit Tracker\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Vehicle Positions Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load vehicle positions from both files and combine them\n",
        "vehicle_positions_file1 = 'vehicle_positions_rt_rows-11-13.csv'\n",
        "vehicle_positions_file2 = 'vehicle_positions_rt_rows_fri_11:14.csv'\n",
        "\n",
        "print(f\"Loading {vehicle_positions_file1}...\")\n",
        "vehicle_positions1 = load_vehicle_positions(data_dir / vehicle_positions_file1)\n",
        "print(f\"  Loaded {len(vehicle_positions1):,} records\")\n",
        "\n",
        "print(f\"\\nLoading {vehicle_positions_file2}...\")\n",
        "vehicle_positions2 = load_vehicle_positions(data_dir / vehicle_positions_file2)\n",
        "print(f\"  Loaded {len(vehicle_positions2):,} records\")\n",
        "\n",
        "# Combine both dataframes\n",
        "vehicle_positions = pd.concat([vehicle_positions1, vehicle_positions2], ignore_index=True)\n",
        "print(f\"\\nCombined: {len(vehicle_positions):,} total records\")\n",
        "\n",
        "print(f\"\\nVehicle Positions Shape: {vehicle_positions.shape}\")\n",
        "print(f\"\\nColumns: {list(vehicle_positions.columns)}\")\n",
        "print(f\"\\nFirst few rows:\")\n",
        "vehicle_positions.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic statistics\n",
        "print(\"\\n=== Vehicle Positions Statistics ===\")\n",
        "print(f\"Total records: {len(vehicle_positions):,}\")\n",
        "print(f\"Unique routes: {vehicle_positions['route_id'].nunique()}\")\n",
        "print(f\"Unique trips: {vehicle_positions['trip_id'].nunique()}\")\n",
        "print(f\"Unique vehicles: {vehicle_positions['vehicle_id'].nunique()}\")\n",
        "print(f\"Unique stops: {vehicle_positions['stop_id'].nunique()}\")\n",
        "print(f\"\\nDate range: {vehicle_positions['datetime'].min()} to {vehicle_positions['datetime'].max()}\")\n",
        "print(f\"\\nRoute distribution (top 10):\")\n",
        "vehicle_positions['route_id'].value_counts().head(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load GTFS Stop Times Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load GTFS data\n",
        "gtfs_data = load_gtfs_data(data_dir)\n",
        "print(\"Loaded GTFS files:\", list(gtfs_data.keys()))\n",
        "\n",
        "# Preprocess GTFS\n",
        "gtfs_data = preprocess_gtfs(gtfs_data)\n",
        "\n",
        "# Explore stop_times\n",
        "if 'stop_times' in gtfs_data:\n",
        "    stop_times = gtfs_data['stop_times']\n",
        "    print(f\"\\nStop Times Shape: {stop_times.shape}\")\n",
        "    print(f\"\\nStop Times Columns: {list(stop_times.columns)}\")\n",
        "    print(f\"\\nUnique trips in stop_times: {stop_times['trip_id'].nunique()}\")\n",
        "    print(f\"\\nUnique stops in stop_times: {stop_times['stop_id'].nunique()}\")\n",
        "    stop_times.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Merge Vehicle Positions with Stop Times\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Merge data (using a sample for faster exploration)\n",
        "# Take a sample of vehicle positions for exploration\n",
        "sample_size = min(10000, len(vehicle_positions))\n",
        "vehicle_sample = vehicle_positions.sample(n=sample_size, random_state=42)\n",
        "\n",
        "# Merge with stop_times\n",
        "merged_data = merge_vehicle_positions_with_stop_times(\n",
        "    vehicle_sample,\n",
        "    gtfs_data['stop_times']\n",
        ")\n",
        "\n",
        "print(f\"Merged data shape: {merged_data.shape}\")\n",
        "print(f\"\\nMerge success rate: {(merged_data['arrival_time'].notna().sum() / len(merged_data) * 100):.1f}%\")\n",
        "print(f\"\\nMerged columns: {list(merged_data.columns)}\")\n",
        "merged_data.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Extract Features and Explore Delays\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract all features\n",
        "features_df = extract_all_features(merged_data)\n",
        "\n",
        "print(f\"Features shape: {features_df.shape}\")\n",
        "print(f\"\\nFeature columns: {len(features_df.columns)} columns\")\n",
        "\n",
        "# Check delay statistics if available\n",
        "if 'arrival_delay_minutes' in features_df.columns:\n",
        "    delays = features_df['arrival_delay_minutes'].dropna()\n",
        "    print(f\"\\n=== Delay Statistics ===\")\n",
        "    print(f\"Mean delay: {delays.mean():.2f} minutes\")\n",
        "    print(f\"Median delay: {delays.median():.2f} minutes\")\n",
        "    print(f\"Std delay: {delays.std():.2f} minutes\")\n",
        "    print(f\"Min delay: {delays.min():.2f} minutes\")\n",
        "    print(f\"Max delay: {delays.max():.2f} minutes\")\n",
        "    print(f\"On-time percentage (Â±1 min): {((delays >= -1) & (delays <= 1)).sum() / len(delays) * 100:.1f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Visualizations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot delay distribution\n",
        "if 'arrival_delay_minutes' in features_df.columns:\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    \n",
        "    plt.subplot(1, 2, 1)\n",
        "    delays = features_df['arrival_delay_minutes'].dropna()\n",
        "    plt.hist(delays, bins=50, edgecolor='black', alpha=0.7)\n",
        "    plt.xlabel('Delay (minutes)')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.title('Distribution of Arrival Delays')\n",
        "    plt.axvline(x=0, color='r', linestyle='--', label='On Time')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.subplot(1, 2, 2)\n",
        "    if 'hour' in features_df.columns:\n",
        "        hourly_delays = features_df.groupby('hour')['arrival_delay_minutes'].mean()\n",
        "        plt.plot(hourly_delays.index, hourly_delays.values, marker='o')\n",
        "        plt.xlabel('Hour of Day')\n",
        "        plt.ylabel('Average Delay (minutes)')\n",
        "        plt.title('Average Delay by Hour')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.xticks(range(24))\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Status breakdown\n",
        "if 'current_status' in features_df.columns:\n",
        "    status_counts = features_df['current_status'].value_counts()\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    status_counts.plot(kind='bar')\n",
        "    plt.xlabel('Status')\n",
        "    plt.ylabel('Count')\n",
        "    plt.title('Vehicle Status Distribution')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\nStatus breakdown:\")\n",
        "    print(status_counts)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Save Processed Data for Next Steps\n",
        "\n",
        "Save a sample of the processed data for feature engineering and model training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save features for next notebook\n",
        "# For full dataset, remove .sample() and process all data\n",
        "features_df.to_csv(processed_dir / 'explored_features.csv', index=False)\n",
        "print(f\"Saved features to {processed_dir / 'explored_features.csv'}\")\n",
        "print(f\"Shape: {features_df.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
